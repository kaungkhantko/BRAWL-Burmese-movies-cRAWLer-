# ğŸ“‘ Burmese Movies Catalogue â€“ Developer Documentation

## ğŸ“Œ Scope

This document supports developers working on the **Burmese Movies Catalogue crawler** by outlining its validation pipeline, mock mode, and test strategy.

It complements the following key documents:

- [`README.md`](../README.md) â€” high-level project vision, features, usage
- [`architecture.md`](architecture.md) â€” current & future system design, diagrams, pain-points
- [`requirements_strategy_execution.md`](requirements_strategy_execution.md) â€” functional goals, constraints, KPIs

---

## ğŸ§ª Pydantic-Based Validation

All scraped items are validated using **Pydantic** before being stored or exported.

### ğŸ“‹ Key Validation Rules

- `title` is required and cannot be blank
- `year` must be <= current year + 5 years 
- `poster_url` and `streaming_link` must be valid URLs
- `cast` must be either a comma-separated string or list of strings
- `synopsis` must be â‰¤ 1000 characters
- Leading/trailing whitespace is stripped from all string fields

### ğŸ“ Schema Location

- **Validation Logic**: `burmese_movies_crawler/schema/item_schema.py`

---

## ğŸ§ª Validation Flow Summary

```text
ğŸ•·ï¸  Crawl (Scrapy Spider)
      â†“
ğŸ§±  Parse HTML Content
      â†“
ğŸ“¦  Build Scrapy Item (BurmeseMoviesItem)
      â†“
âœ…  Pydantic Validation (MovieItem Schema)
    â”œâ”€ Valid â†’ Pass to Pipeline
    â””â”€ Invalid â†’ Drop & Log Warning
      â†“
ğŸ§¹  Transform / Normalize (optional enrichments)
      â†“
ğŸ“  Store
````

---

## ğŸ§ª Testing

Run tests with:

```bash
pytest tests/
```

---

## ğŸ§ª Mock Mode (Offline Testing)

Mock Mode allows you to run the crawler entirely offline, simulating real runs using saved HTML files.

### ğŸ’¡ Why Use It?

* Enables deterministic testing without network/Selenium
* CI-friendly and lightweight
* Useful for debugging extraction and selectors

### â–¶ï¸ How to Enable

```bash
MOCK_MODE=true python run_spider.py
```

### ğŸ“ How It Works

* Replaces all network/Selenium requests with local files
* Matches saved HTML fixtures based on MD5 hash of original URL

### ğŸ· Fixture Naming Convention

```python
import hashlib
print(hashlib.md5("https://example.com".encode()).hexdigest())
```

Save the corresponding file as:

```text
tests/fixtures/<md5_hash>.html
```

---

## ğŸ” Related Files

* `run_spider.py` â€“ Entry point for the crawler
* `tests/fixtures/` â€“ Directory for local HTML test pages
* `tests/` â€“ Test files
* `output/` â€“ Generated results from each run

---

## ğŸ§­ Whatâ€™s Not in This File

* Strategic rationale: see [`README.md`](../README.md)
* Architecture diagrams: see [`architecture.md`](architecture.md)
* System goals & KPIs: see [`requirements_strategy_execution.md`](requirements_strategy_execution.md)

---

## Next Steps (For this documentation file)

1. **Expand Validation Documentation**
   * Add examples of common validation errors and troubleshooting
   * Document custom validators and field transformers
   * Create validation rule reference table with rationale

2. **Enhance Mock Mode Guide**
   * Add step-by-step tutorial for creating new fixtures
   * Document fixture organization best practices
   * Include examples of fixture-based test cases

3. **Add Developer Workflow Section**
   * Local development setup instructions
   * Code contribution guidelines
   * PR review process

4. **Create Troubleshooting Guide**
   * Common extraction failures and solutions
   * Debugging validation pipeline issues
   * Performance optimization tips

5. **Add API Documentation**
   * Document crawler output formats (JSON, CSV)
   * Explain field normalization rules
   * Provide sample output schemas

6. **Include CI/CD Pipeline Documentation**
   * Test coverage requirements
   * Automated validation checks
   * Release process