import logging
import re
import functools
from urllib.parse import urldefrag
from fuzzywuzzy import process
from burmese_movies_crawler.items import BurmeseMoviesItem
from .link_utils import is_valid_link
from .field_mapping_loader import load_field_mapping
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

# Compiled regex pattern for text cleaning
CLEAN_TEXT_PATTERN = re.compile(r'[:\-â€“]', re.UNICODE)

# Default confidence threshold
DEFAULT_THRESHOLD = 70

# Custom exceptions for error handling
class FieldExtractorError(Exception):
    """Base exception for all FieldExtractor errors"""
    pass

class InitializationError(FieldExtractorError):
    """Error during initialization of FieldExtractor"""
    pass

class ExtractionError(FieldExtractorError):
    """Error during data extraction"""
    pass

class ProcessingError(FieldExtractorError):
    """Error during data processing"""
    pass

class TableProcessingError(FieldExtractorError):
    """Error during table processing"""
    pass

"""
flowchart TD
    %% Main component structure with clear boundaries
    subgraph Core["FieldExtractor Core"]
        A["FieldExtractor"]
        A --> B["load_field_mapping()"]
        B --> C["Content Type: 'movies'"]
    end
    
    %% Public API methods with clear entry/exit points
    subgraph API["Public Extraction API"]
        direction TB
        A --> D["extract_links()"]
        A --> E["extract_main_fields()"]
        A --> F["extract_paragraphs()"]
        A --> G["extract_from_table()"]
        
        D -->|"return"| O1["URLs[]"]
        E -->|"return"| O2["Fields{}"]
        F -->|"return"| O3["Fields{}"]
        G -->|"yield"| O4["Items[]"]
    end
    
    %% Link extraction flow
    subgraph LinkExtraction["Link Processing"]
        direction TB
        D --> D1["CSS selection"]
        D1 -->|"for each link"| D2["URL normalization"]
        D2 --> D3["Validation"]
        
        %% Decision point
        D3 -->|"valid"| D4["Add to results"]
        D3 -->|"invalid"| D5["Skip"]
    end
    
    %% Field extraction flow
    subgraph FieldExtraction["Field Processing"]
        direction TB
        E --> E1["extract_field_value()"]
        F --> F1["paragraph extraction"]
        F1 -->|"for each paragraph"| F2["field matching"]
        
        G --> G1["header extraction"]
        G1 --> G2["row processing"]
        G2 -->|"for each row"| G3["cell mapping"]
    end
    
    %% Shared utilities with clear dependencies
    subgraph Utilities["Shared Utilities"]
        direction TB
        H1["_match_field()"]
        H2["_clean_text()"]
        H3["_map_headers()"]
        
        %% Show caching explicitly
        H1 -->|"@lru_cache"| H1a["fuzzy matching"]
        H2 -->|"@lru_cache"| H2a["text cleaning"]
        H3 --> H3a["header mapping"]
        
        %% Connect utilities to their callers
        F2 --> H1
        G2 --> H3
        H3 --> H1
        F2 --> H2
        E1 --> H2
    end
    
    %% Error handling as a separate concern
    subgraph ErrorHandling["Error Handling"]
        direction TB
        Z1["FieldExtractorError"]
        Z1 --> Z2["InitializationError"]
        Z1 --> Z3["ExtractionError"]
        Z1 --> Z4["ProcessingError"]
        Z1 --> Z5["TableProcessingError"]
    end
    
    %% Connect error handling to main components
    Core --> ErrorHandling
    API --> ErrorHandling
"""

class FieldExtractor:
    def __init__(self, invalid_links=None, content_type="movies"):
        try:
            self.invalid_links = invalid_links if invalid_links is not None else []
            self.items_scraped = 0
            
            # Load field mappings with error handling
            try:
                self.label_mapping = load_field_mapping(content_type)
            except Exception as e:
                logger.error(f"Failed to load field mapping for {content_type}: {str(e)}")
                raise InitializationError(f"Failed to load field mapping: {str(e)}") from e
            
            # Cache for fuzzy matching results
            self._field_match_cache = {}
            self._header_map_cache = {}
            
            # Pre-compute field patterns for faster matching
            self._field_patterns = {}
            for field, meta in self.label_mapping.items():
                if not isinstance(meta, dict) or 'labels' not in meta:
                    logger.warning(f"Invalid field mapping for {field}, skipping")
                    continue
                    
                self._field_patterns[field] = {
                    'labels': meta['labels'],
                    'threshold': meta.get('confidence_threshold', DEFAULT_THRESHOLD)
                }
                
            if not self._field_patterns:
                logger.warning("No valid field patterns found")
                
        except Exception as e:
            if not isinstance(e, InitializationError):
                logger.error(f"Initialization error: {str(e)}")
                raise InitializationError(f"Failed to initialize FieldExtractor: {str(e)}") from e
            raise

    def extract_links(self, response):
        try:
            if not response:
                logger.error("No response object provided")
                raise ExtractionError("No response object provided")

            combined_selector = (
                'div.item a::attr(href), div.card a::attr(href), div.movie a::attr(href), '
                'div.movie-entry a::attr(href), div.movie-card a::attr(href), article a::attr(href)'
            )

            try:
                # Always combine specific and generic selectors
                primary = response.css(combined_selector).getall()
                fallback = response.css('a::attr(href)').getall()
                links = list(set(primary) | set(fallback))
            except Exception as e:
                logger.error(f"Failed to extract links with CSS selector: {str(e)}")
                raise ExtractionError(f"CSS selector error: {str(e)}") from e

            unique_links = set()
            for link in links:
                try:
                    if not isinstance(link, str):
                        continue

                    raw = link.strip()

                    # Normalize before validation
                    resolved = urljoin(response.url, raw)
                    clean = urldefrag(resolved)[0]

                    if is_valid_link(clean, self.invalid_links):
                        unique_links.add(clean)

                except Exception as e:
                    logger.warning(f"Error processing link '{link}': {str(e)}")

            logger.info(f"Extracted {len(unique_links)} valid links after filtering.")
            return list(unique_links)

        except Exception as e:
            if not isinstance(e, ExtractionError):
                logger.error(f"Link extraction error: {str(e)}")
                raise ExtractionError(f"Failed to extract links: {str(e)}") from e
            raise

    def extract_first(self, response, selectors):
        try:
            if not response:
                logger.error("No response object provided")
                raise ExtractionError("No response object provided")
                
            if not selectors:
                logger.warning("No selectors provided")
                return None
                
            # Create a combined selector for more efficient querying
            if len(selectors) > 1:
                try:
                    combined_selector = ', '.join(selectors)
                    matches = response.css(combined_selector)
                    if matches:
                        value = matches.get()
                        if value:
                            return value.strip()
                except Exception as e:
                    logger.warning(f"Error with combined selector: {str(e)}")
                    # Fall through to individual selectors
            
            # If combined selector didn't work, try individual selectors
            # This is a fallback for complex selectors that can't be combined
            for sel in selectors:
                try:
                    matches = response.css(sel)
                    if matches:
                        value = matches.get()
                        if value:
                            return value.strip()
                        
                        # Fallback: get text content even if it's nested inside
                        try:
                            nested_text = matches.xpath('string()').get()
                            if nested_text and nested_text.strip():
                                return nested_text.strip()
                        except Exception as e:
                            logger.warning(f"Error getting nested text for selector '{sel}': {str(e)}")
                except Exception as e:
                    logger.warning(f"Error with selector '{sel}': {str(e)}")
                    # Continue with next selector
            
            return None
            
        except Exception as e:
            if not isinstance(e, ExtractionError):
                logger.error(f"Extract first error: {str(e)}")
                raise ExtractionError(f"Failed to extract first match: {str(e)}") from e
            raise

    def extract_main_fields(self, response):
        try:
            if not response:
                logger.error("No response object provided")
                raise ExtractionError("No response object provided")
                
            # Define fields with prioritized selectors (most specific first)
            fields = {
                'title': ['h1.entry-title::text', 'h1.title::text', 'div.movie-title::text'],
                'year': ['.ytps::text', 'span[class*="year"]::text'],
                'poster_url': ['div.entry-content img::attr(src)'],
                'streaming_link': ['iframe::attr(src)']
            }
            
            # Process all fields in a batch
            results = {}
            for field, selectors in fields.items():
                try:
                    value = self.extract_first(response, selectors)
                    if value:
                        results[field] = value
                except Exception as e:
                    logger.warning(f"Error extracting field '{field}': {str(e)}")
                    # Continue with next field
                    
            return results
            
        except Exception as e:
            if not isinstance(e, ExtractionError):
                logger.error(f"Main fields extraction error: {str(e)}")
                raise ExtractionError(f"Failed to extract main fields: {str(e)}") from e
            raise

    def extract_paragraphs(self, response):
        try:
            if not response:
                logger.error("No response object provided")
                raise ExtractionError("No response object provided")
                
            data, used = {}, set()
            
            try:
                # Get all paragraphs at once
                paragraphs = response.css('div.entry-content p::text').getall()
            except Exception as e:
                logger.error(f"Failed to extract paragraphs: {str(e)}")
                raise ExtractionError(f"Failed to extract paragraphs: {str(e)}") from e
            
            # Pre-filter: only process paragraphs that are likely to contain field data
            # This avoids processing paragraphs that are too short or too long
            filtered_paragraphs = []
            for p in paragraphs:
                try:
                    p_stripped = p.strip()
                    if 5 <= len(p_stripped) <= 200:
                        filtered_paragraphs.append(p_stripped)
                except Exception as e:
                    logger.warning(f"Error processing paragraph: {str(e)}")
                    # Continue with next paragraph
            
            # Process paragraphs in batches
            for clean in filtered_paragraphs:
                try:
                    # Use cached field matching
                    field, score = self._match_field(clean)
                    if field and field not in used and score > DEFAULT_THRESHOLD:
                        data[field] = self._clean_text(clean)
                        used.add(field)
                except Exception as e:
                    logger.warning(f"Error matching field for text '{clean[:30]}...': {str(e)}")
                    # Continue with next paragraph
                    
            return data
            
        except Exception as e:
            if not isinstance(e, ExtractionError):
                logger.error(f"Paragraph extraction error: {str(e)}")
                raise ProcessingError(f"Failed to process paragraphs: {str(e)}") from e
            raise

    def extract_from_table(self, response, table):
        try:
            if not response or not table:
                logger.error("No response or table object provided")
                raise TableProcessingError("No response or table object provided")
                
            # Extract headers once and validate
            try:
                headers = [h.strip() for h in table.css('thead th::text, thead td::text').getall()]
                if not headers:
                    # Fallback if <thead> is missing
                    headers = [h.strip() for h in table.css('tr:first-child th::text, tr:first-child td::text').getall()]
            except Exception as e:
                logger.error(f"Failed to extract table headers: {str(e)}")
                raise TableProcessingError(f"Failed to extract table headers: {str(e)}") from e
            
            if not headers:
                logger.warning("No headers found in table, skipping extraction")
                return
                
            # Cache header mapping
            try:
                table_id = id(table)
                if table_id not in self._header_map_cache:
                    self._header_map_cache[table_id] = self._map_headers(headers)
                header_map = self._header_map_cache[table_id]
            except Exception as e:
                logger.error(f"Failed to map headers: {str(e)}")
                raise TableProcessingError(f"Failed to map headers: {str(e)}") from e
            
            # Process rows in batches
            try:
                rows = table.css('tbody tr')
            except Exception as e:
                logger.error(f"Failed to get table rows: {str(e)}")
                raise TableProcessingError(f"Failed to get table rows: {str(e)}") from e
            
            for row in rows:
                try:
                    cells = [c.strip() for c in row.css('td::text, td *::text').getall() if c.strip()]
                    
                    # Handle partial matches gracefully
                    if cells:
                        item = BurmeseMoviesItem()
                        # Map available cells to headers
                        for i, value in enumerate(cells):
                            if i < len(headers):
                                field = header_map.get(headers[i])
                                if field:
                                    item[field] = value
                        
                        if any(item.values()):
                            yield item
                            self.items_scraped += 1
                except Exception as e:
                    logger.warning(f"Error processing table row: {str(e)}")
                    # Continue with next row
                    
        except Exception as e:
            if not isinstance(e, TableProcessingError):
                logger.error(f"Table extraction error: {str(e)}")
                raise TableProcessingError(f"Failed to process table: {str(e)}") from e
            raise

    def _map_headers(self, headers):
        try:
            if not headers:
                return {}
                
            results = {}
            
            try:
                # Create a cache key for this set of headers
                headers_key = tuple(sorted(h.lower() for h in headers if h))
                
                # Check if we already processed these headers
                if headers_key in self._header_map_cache:
                    return self._header_map_cache[headers_key]
            except Exception as e:
                logger.warning(f"Error creating headers cache key: {str(e)}")
                # Continue without caching
                
            # Process each header
            for head in headers:
                try:
                    if not head:
                        continue
                        
                    head_lower = head.lower()
                    
                    # Check if this individual header is in cache
                    if head_lower in self._field_match_cache:
                        field = self._field_match_cache[head_lower]
                        if field:
                            results[head] = field
                        continue
                        
                    # Find best match for this header
                    best_field, best_score = None, 0
                    
                    for field, pattern in self._field_patterns.items():
                        try:
                            if not pattern.get('labels'):
                                continue
                                
                            match, score = process.extractOne(head_lower, pattern['labels'])
                            threshold = pattern.get('threshold', DEFAULT_THRESHOLD)
                            
                            if score >= threshold and score > best_score:
                                best_field, best_score = field, score
                        except Exception as e:
                            logger.warning(f"Error matching header '{head}' to field '{field}': {str(e)}")
                            # Continue with next field
                    
                    # Cache the result
                    self._field_match_cache[head_lower] = best_field
                    if best_field:
                        results[head] = best_field
                except Exception as e:
                    logger.warning(f"Error processing header '{head}': {str(e)}")
                    # Continue with next header
                    
            try:
                # Cache the mapping for these headers
                if headers_key:
                    self._header_map_cache[headers_key] = results
            except Exception as e:
                logger.warning(f"Error caching header mapping: {str(e)}")
                
            return results
            
        except Exception as e:
            logger.error(f"Header mapping error: {str(e)}")
            raise TableProcessingError(f"Failed to map headers: {str(e)}") from e

    @functools.lru_cache(maxsize=128)
    def _match_field(self, text):
        """Match text to a field using fuzzy matching with caching."""
        try:
            if not text:
                return None, 0
                
            # Check if we already processed this text
            text_lower = text.lower()
            if text_lower in self._field_match_cache:
                return self._field_match_cache[text_lower]
                
            best, score = None, 0
            
            # Use pre-computed field patterns for faster matching
            for field, pattern in self._field_patterns.items():
                try:
                    if not pattern.get('labels'):
                        continue
                        
                    match, match_score = process.extractOne(text_lower, pattern['labels'])
                    threshold = pattern['threshold']
                    
                    if match_score > score and match_score >= threshold:
                        best, score = field, match_score
                except Exception as e:
                    logger.warning(f"Error matching field '{field}': {str(e)}")
                    # Continue with next field
                    
            # Cache the result
            self._field_match_cache[text_lower] = (best, score)
            return best, score
            
        except Exception as e:
            logger.error(f"Field matching error for text '{text[:30]}...': {str(e)}")
            raise ProcessingError(f"Failed to match field: {str(e)}") from e

    @functools.lru_cache(maxsize=128)
    def _clean_text(self, text):
        """Clean text using efficient regex operations with caching."""
        try:
            if not text:
                return ""
                
            # Replace non-breaking space
            text = text.replace('\xa0', ' ')
            
            try:
                # Use pre-compiled regex pattern for splitting
                parts = CLEAN_TEXT_PATTERN.split(text, maxsplit=1)
                
                # Extract value part efficiently
                return parts[-1].strip() if len(parts) > 1 else text.strip()
            except Exception as e:
                logger.warning(f"Error cleaning text with regex: {str(e)}")
                # Fallback to simple cleaning
                return text.strip()
                
        except Exception as e:
            logger.error(f"Text cleaning error: {str(e)}")
            raise ProcessingError(f"Failed to clean text: {str(e)}") from e
